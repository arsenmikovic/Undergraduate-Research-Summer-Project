{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5yWlo1GQ82c-"},"outputs":[],"source":["# AUTOGENERATED! DO NOT EDIT! File to edit: ../transformer.ipynb.\n","\n","# %% auto 0\n","__all__ = ['Config', 'HookPoint', 'Embed', 'Unembed', 'PosEmbed', 'LayerNorm', 'Attention', 'MLP', 'TransformerBlock',\n","           'Transformer', 'make_fourier_basis', 'calculate_key_freqs', 'get_components_of_trig_loss',\n","           'calculate_excluded_loss', 'calculate_trig_loss', 'calculate_coefficients', 'gen_train_test', 'full_loss',\n","           'Trainer', 'train_model']\n","\n","# %% ../transformer.ipynb 3\n","import numpy as np\n","import torch as t\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import torch.nn.functional as F\n","import einops\n","import random\n","import helpers\n","from dataclasses import dataclass\n","import os\n","import wandb\n","\n","# %% ../transformer.ipynb 4\n","# TODO does dataclass really require type annotations lol\n","\n","\n","@dataclass(frozen = True)\n","class Config():\n","    lr: float = 5e-4 #@param\n","    weight_decay: float = 1.0 #@param\n","    p: int = 8 #@param\n","    d_model: int = 8 #@param\n","    fn_name: str = 'divide_normal' #@param ['add', 'subtract', 'x2xyy2','rand', 'divide', 'divide_normal', 'multiply']\n","    frac_train: float = 0.3 #@param\n","    num_epochs: int = 1000 #@param\n","    save_models: bool = False #@param\n","    save_every: int = 100 #@param\n","\n","    # TODO for the first 1000 steps, save every 10 because 'interesting stuff happens at the start'\n","    # TODO add a helper function to generate indices here\n","\n","    # Stop training when test loss is <stopping_thresh\n","    stopping_thresh: int = -1 #@param\n","    seed: int = 0 #@param\n","\n","    num_layers: int = 1\n","    batch_style: str = 'full'\n","    d_vocab: int = p+1\n","    n_ctx: int = 3\n","    d_mlp: int = 4*d_model\n","    num_heads: int = 4\n","\n","    act_type: str = 'ReLU' #@param ['ReLU', 'GeLU']\n","\n","\n","    device: t.device = t.device(\"cuda\")\n","\n","    # TODO ankify the privileged basis concept- a priori vs etc. ; consider writing up an explanation of privileged basis\n","\n","    use_ln: bool = False\n","\n","    take_metrics_every_n_epochs: int = 1000 #@param\n","    thresholds: tuple = (1.10, 1.30, 1.50, 1.80, 2.20, 3.20, 10.0)\n","\n","    @property\n","    def thresh_tensor(self):\n","        # convenient Torch version (with +inf at the end)\n","        return t.tensor(list(self.thresholds) + [float('inf')], device=self.device)\n","\n","    @property\n","    def d_head(self):\n","        return self.d_model // self.num_heads\n","\n","    @property\n","    def random_answers(self):\n","        return np.random.randint(low=0, high=self.p, size=(self.p, self.p))\n","\n","    @property\n","    def fns_dict(self):\n","        return {\n","            'add': lambda x,y:(x+y) % self.p,\n","            'subtract': lambda x,y:(x-y) % self.p,\n","            'x2xyy2': lambda x,y:(x**2+x*y+y**2) % self.p,\n","            'rand': lambda x,y:self.random_answers[x][y],\n","            'divide': lambda x, y: 0 if y == 0 else (\n","                            0 if x / y < 1.7 else\n","                            1 if x / y < 2.5 else\n","                            2\n","                            #2 if x / y < 1.50 else\n","                            #3 if x / y < 1.80 else\n","                            #4 if x / y < 2.20 else\n","                            #5 if x / y < 3.20 else\n","                            #6 if x / y < 10.0 else\n","                            #7\n","                            ),\n","            'multiply': lambda x,y: (x*y)%self.p,\n","            'divide_normal': lambda x,y: x//y\n","            }\n","\n","    @property\n","    def fn(self):\n","        return self.fns_dict[self.fn_name]\n","\n","    def is_train_is_test(self, train):\n","        '''Creates an array of Boolean indices according to whether each data point is in train or test.\n","        Used to index into the big batch of all possible data'''\n","        # TODO probably the wrong place for this\n","        is_train = []\n","        is_test = []\n","        for x in range(self.p):\n","            for y in range(self.p):\n","                if (x, y, 113) in train:\n","                    is_train.append(True)\n","                    is_test.append(False)\n","                else:\n","                    is_train.append(False)\n","                    is_test.append(True)\n","        is_train = np.array(is_train)\n","        is_test = np.array(is_test)\n","        return (is_train, is_test)\n","\n","    def is_it_time_to_save(self, epoch):\n","        return (epoch % self.save_every == 0)\n","\n","    def is_it_time_to_take_metrics(self, epoch):\n","        return epoch % self.take_metrics_every_n_epochs == 0\n","\n","# TODO make this an assert inside the consturctor\n","assert Config.d_model % Config.num_heads == 0\n","\n","# %% ../transformer.ipynb 5\n","class HookPoint(nn.Module):\n","    '''A helper class to get access to intermediate activations (inspired by Garcon)\n","    It's a dummy module that is the identity function by default\n","    I can wrap any intermediate activation in a HookPoint and get a convenient way to add PyTorch hooks\n","    '''\n","    def __init__(self):\n","        super().__init__()\n","        self.fwd_hooks = []\n","        self.bwd_hooks = []\n","\n","    def give_name(self, name):\n","        # Called by the model at initialisation\n","        self.name = name\n","\n","    def add_hook(self, hook, dir='fwd'):\n","        # Hook format is fn(activation, hook_name)\n","        # Change it into PyTorch hook format (this includes input and output,\n","        # which are the same for a HookPoint)\n","        def full_hook(module, module_input, module_output):\n","            return hook(module_output, name=self.name)\n","        if dir=='fwd':\n","            handle = self.register_forward_hook(full_hook)\n","            self.fwd_hooks.append(handle)\n","        elif dir=='bwd':\n","            handle = self.register_backward_hook(full_hook)\n","            self.bwd_hooks.append(handle)\n","        else:\n","            raise ValueError(f\"Invalid direction {dir}\")\n","\n","    def remove_hooks(self, dir='fwd'):\n","        if (dir=='fwd') or (dir=='both'):\n","            for hook in self.fwd_hooks:\n","                hook.remove()\n","            self.fwd_hooks = []\n","        if (dir=='bwd') or (dir=='both'):\n","            for hook in self.bwd_hooks:\n","                hook.remove()\n","            self.bwd_hooks = []\n","        if dir not in ['fwd', 'bwd', 'both']:\n","            raise ValueError(f\"Invalid direction {dir}\")\n","\n","    def forward(self, x):\n","        return x\n","\n","# %% ../transformer.ipynb 6\n","class Embed(nn.Module):\n","    '''Define network architecture\n","    I defined my own transformer from scratch so I'd fully understand each component\n","    - I expect this wasn't necessary or particularly important, and a bunch of this replicates existing Pyt functionality\n","    '''\n","    def __init__(self, d_vocab, d_model):\n","        super().__init__()\n","        self.W_E = nn.Parameter(t.randn(d_model, d_vocab)/np.sqrt(d_model))\n","\n","    def forward(self, x):\n","        return t.einsum('dbp -> bpd', self.W_E[:, x])\n","\n","#| export\n","class Unembed(nn.Module):\n","    def __init__(self, d_vocab, d_model):\n","        super().__init__()\n","        self.W_U = nn.Parameter(t.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n","\n","    def forward(self, x):\n","        return (x @ self.W_U)\n","\n","#| export\n","class PosEmbed(nn.Module):\n","    def __init__(self, max_ctx, d_model):\n","        super().__init__()\n","        self.W_pos = nn.Parameter(t.randn(max_ctx, d_model)/np.sqrt(d_model))\n","\n","    def forward(self, x):\n","        return x+self.W_pos[:x.shape[-2]]\n","\n","#| export\n","class LayerNorm(nn.Module):\n","    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n","        super().__init__()\n","        self.model = model\n","        self.w_ln = nn.Parameter(t.ones(d_model))\n","        self.b_ln = nn.Parameter(t.zeros(d_model))\n","        self.epsilon = epsilon\n","\n","    def forward(self, x):\n","        if self.model[0].use_ln:\n","            x = x - x.mean(axis=-1)[..., None]\n","            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n","            x = x * self.w_ln\n","            x = x + self.b_ln\n","            return x\n","        else:\n","            return x\n","\n","#| export\n","class Attention(nn.Module):\n","    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n","        super().__init__()\n","        self.model = model\n","        self.W_K = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n","        self.W_Q = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n","        self.W_V = nn.Parameter(t.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n","        self.W_O = nn.Parameter(t.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n","        self.register_buffer('mask', t.tril(t.ones((n_ctx, n_ctx))))\n","        self.d_head = d_head\n","        self.hook_k = HookPoint()\n","        self.hook_q = HookPoint()\n","        self.hook_v = HookPoint()\n","        self.hook_z = HookPoint()\n","        self.hook_attn = HookPoint()\n","        self.hook_attn_pre = HookPoint()\n","\n","    def forward(self, x):\n","        k = self.hook_k(t.einsum('ihd,bpd->biph', self.W_K, x))\n","        q = self.hook_q(t.einsum('ihd,bpd->biph', self.W_Q, x))\n","        v = self.hook_v(t.einsum('ihd,bpd->biph', self.W_V, x))\n","        attn_scores_pre = t.einsum('biph,biqh->biqp', k, q)\n","        attn_scores_masked = t.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n","        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n","        z = self.hook_z(t.einsum('biph,biqp->biqh', v, attn_matrix))\n","        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n","        out = t.einsum('df,bqf->bqd', self.W_O, z_flat)\n","        return out\n","\n","#| export\n","class MLP(nn.Module):\n","    def __init__(self, d_model, d_mlp, act_type, model):\n","        super().__init__()\n","        self.model = model\n","        self.W_in = nn.Parameter(t.randn(d_mlp, d_model)/np.sqrt(d_model))\n","        self.b_in = nn.Parameter(t.zeros(d_mlp))\n","        self.W_out = nn.Parameter(t.randn(d_model, d_mlp)/np.sqrt(d_model))\n","        self.b_out = nn.Parameter(t.zeros(d_model))\n","        self.act_type = act_type\n","        # self.ln = LayerNorm(d_mlp, model=self.model)\n","        self.hook_pre = HookPoint()\n","        self.hook_post = HookPoint()\n","        assert act_type in ['ReLU', 'GeLU']\n","\n","    def forward(self, x):\n","        x = self.hook_pre(t.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n","        if self.act_type=='ReLU':\n","            x = F.relu(x)\n","        elif self.act_type=='GeLU':\n","            x = F.gelu(x)\n","        x = self.hook_post(x)\n","        x = t.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n","        return x\n","\n","# export\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n","        super().__init__()\n","        self.model = model\n","        # self.ln1 = LayerNorm(d_model, model=self.model)\n","        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n","        #self.use_ln = use_ln\n","        # self.ln2 = LayerNorm(d_model, model=self.model)\n","        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n","        self.hook_attn_out = HookPoint()\n","        self.hook_mlp_out = HookPoint()\n","        self.hook_resid_pre = HookPoint()\n","        self.hook_resid_mid = HookPoint()\n","        self.hook_resid_post = HookPoint()\n","\n","    def forward(self, x):\n","        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n","        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n","        #x = self.ln(x)\n","        return x\n","\n","#| export\n","class Transformer(nn.Module):\n","    def __init__(self, config: Config, use_cache=False, use_ln=True):\n","        '''this function could be augmented to contain more options for creating different architectures'''\n","        super().__init__()\n","        self.cache = {}\n","        self.use_cache = use_cache\n","        self.embed = Embed(d_vocab = config.d_vocab, d_model = config.d_model)\n","        self.pos_embed = PosEmbed(max_ctx = config.n_ctx, d_model = config.d_model)\n","        self.blocks = nn.ModuleList([TransformerBlock(d_model = config.d_model,\n","            d_mlp = config.d_mlp,\n","            d_head = config.d_head,\n","            num_heads = config.num_heads,\n","            n_ctx = config.n_ctx,\n","            act_type = config.act_type,\n","            model=[self]) for i in range(config.num_layers)])\n","        self.unembed = Unembed(d_vocab = 8, d_model = config.d_model)\n","        #self.use_ln = use_ln\n","\n","        for name, module in self.named_modules():\n","            if type(module)==HookPoint:\n","                module.give_name(name)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        x = self.pos_embed(x)\n","        for block in self.blocks:\n","            x = block(x)\n","        # x = self.ln(x)\n","        x = self.unembed(x)\n","        return x\n","\n","    def set_use_cache(self, use_cache):\n","        self.use_cache = use_cache\n","\n","    def hook_points(self):\n","        return [module for name, module in self.named_modules() if 'hook' in name]\n","\n","    def remove_all_hooks(self):\n","        for hp in self.hook_points():\n","            hp.remove_hooks('fwd')\n","            hp.remove_hooks('bwd')\n","\n","    def cache_all(self, cache, incl_bwd=False):\n","        # Caches all activations wrapped in a HookPoint\n","        def save_hook(tensor, name):\n","            cache[name] = tensor.detach()\n","        def save_hook_back(tensor, name):\n","            cache[name+'_grad'] = tensor[0].detach()\n","        for hp in self.hook_points():\n","            hp.add_hook(save_hook, 'fwd')\n","            if incl_bwd:\n","                hp.add_hook(save_hook_back, 'bwd')\n","\n","# %% ../transformer.ipynb 7\n","def make_fourier_basis(config: Config):\n","    fourier_basis = []\n","    fourier_basis.append(t.ones(config.p)/np.sqrt(config.p))\n","    fourier_basis_names = ['Const']\n","    # Note that if p is even, we need to explicitly add a term for cos(kpi), ie\n","    # alternating +1 and -1\n","    for i in range(1, config.p//2 +1):\n","        fourier_basis.append(t.cos(2*t.pi*t.arange(config.p)*i/config.p))\n","        fourier_basis.append(t.sin(2*t.pi*t.arange(config.p)*i/config.p))\n","        fourier_basis[-2]/=fourier_basis[-2].norm()\n","        fourier_basis[-1]/=fourier_basis[-1].norm()\n","        fourier_basis_names.append(f'cos {i}')\n","        fourier_basis_names.append(f'sin {i}')\n","    return t.stack(fourier_basis, dim=0).to(config.device)\n","\n","\n","def calculate_key_freqs(config: Config, model: Transformer, all_data):\n","    # TODO this was moved from the app code; probably move it around\n","    labels = t.tensor([config.fn(i, j) for i, j, _ in all_data]).to(config.device)\n","    cache = {}\n","    model.remove_all_hooks() # TODO is this line fucky??\n","    model.cache_all(cache)\n","    model(all_data)\n","    neuron_acts = cache['blocks.0.mlp.hook_post'][:, -1]\n","    # Center the neurons to remove the constant term\n","    neuron_acts_centered = neuron_acts - einops.reduce(neuron_acts, 'batch neuron -> 1 neuron', 'mean')\n","    # Note that fourier_neuron_acts[(0, 0), i]==0 for all i, because we centered the activations\n","    fourier_basis = make_fourier_basis(config = config)\n","    fourier_neuron_acts = helpers.fft2d(neuron_acts_centered, p = config.p, fourier_basis=fourier_basis)\n","\n","    fourier_neuron_acts_square = fourier_neuron_acts.reshape(config.p, config.p, config.d_mlp)\n","    neuron_freqs = []\n","    neuron_frac_explained = []\n","    for ni in range(config.d_mlp):\n","        best_frac_explained = -1e6\n","        best_freq = -1\n","        for freq in range(1, config.p//2):\n","            # We extract the linear and quadratic fourier terms of frequency freq,\n","            # and look at how much of the variance of the full vector this explains\n","            # If neurons specialise into specific frequencies, one frequency should\n","            # have a large value\n","            numerator = helpers.extract_freq_2d(fourier_neuron_acts_square[:, :, ni], freq, p = config.p).pow(2).sum()\n","            denominator = fourier_neuron_acts_square[:, :, ni].pow(2).sum().item()\n","            frac_explained = numerator / denominator\n","            if frac_explained > best_frac_explained:\n","                best_freq = freq\n","                best_frac_explained = frac_explained\n","        neuron_freqs.append(best_freq)\n","        neuron_frac_explained.append(best_frac_explained)\n","    neuron_freqs = np.array(neuron_freqs)\n","    neuron_frac_explained = helpers.to_numpy(neuron_frac_explained)\n","    key_freqs, neuron_freq_counts = np.unique(neuron_freqs, return_counts=True)\n","    return key_freqs\n","\n","def get_components_of_trig_loss(logits, freq, fourier_basis):\n","    cos = helpers.get_component_cos_xpy(logits, freq, fourier_basis=fourier_basis)\n","    sin = helpers.get_component_sin_xpy(logits, freq, fourier_basis=fourier_basis)\n","    return cos + sin\n","\n","\n","def calculate_excluded_loss(config: Config, fourier_basis, key_freqs, is_train, is_test, labels, logits):\n","    row = []\n","    for freq in key_freqs:\n","        cos = helpers.get_component_cos_xpy(logits, freq, fourier_basis=fourier_basis)\n","        sin = helpers.get_component_sin_xpy(logits, freq, fourier_basis=fourier_basis)\n","        value = helpers.test_logits(logits - cos - sin, bias_correction=False, mode='train', p = config.p,\n","           is_train = is_train, is_test = is_test, labels = labels)\n","        row.append(value.item())\n","    return row\n","\n","def calculate_trig_loss(config: Config, model, train, logits, key_freqs, fourier_basis, all_data, is_train, is_test, labels, mode='all'):\n","    trig_logits = sum([get_components_of_trig_loss(logits, freq, fourier_basis) for freq in key_freqs])\n","    return helpers.test_logits(trig_logits,\n","                        p = config.p,\n","                        is_train = is_train,\n","                        is_test = is_test,\n","                        labels = labels,\n","                        bias_correction=True,\n","                        original_logits=logits,\n","                        mode=mode)\n","\n","\n","def calculate_coefficients(logits, fourier_basis, key_freqs, p, device):\n","    '''updated version from https://colab.research.google.com/drive/1ScVRL8OCtTFpOHpgfz0PLTFvX4g_YbuN?usp=sharing#scrollTo=WY4nPUDwl9UN\n","    '''\n","    x = t.arange(p)[None, :, None, None]\n","    y = t.arange(p)[None, None, :, None]\n","    z = t.arange(p)[None, None, None, :]\n","    w = t.arange(1, (p//2+1))[:, None, None, None]\n","    coses = t.cos(w*t.pi*2/p * (x + y - z)).to(device)\n","    coses = coses.reshape(p//2, p*p, p)\n","    coses/= coses.pow(2).sum([-2, -1], keepdim=True).sqrt()\n","    cos_coefficients = (coses * logits).sum([-2, -1])\n","    return cos_coefficients\n","\n","# %% ../transformer.ipynb 8\n","# TODO move this into the config?\n","import dataclasses\n","from collections import defaultdict\n","\n","\n","\n","def gen_train_test(config: Config):\n","    '''Generate train and test split'''\n","    num_to_generate = config.p\n","    pairs = [(i, j, num_to_generate) for i in range(1, num_to_generate) for j in range(1, i+1)]\n","    random.seed(config.seed)\n","    random.shuffle(pairs)\n","    div = int(config.frac_train*len(pairs))\n","    return pairs[:div], pairs[div:]\n","\"\"\"\n","\n","def gen_train_test(config: Config):\n","    '''Generate train and test split'''\n","    num_to_generate = config.p\n","    low = num_to_generate//8\n","    high = 7*num_to_generate//8\n","    grid_points = [(i, j) for i in range(num_to_generate) for j in range(num_to_generate)]\n","    pairs = []\n","    #IMPORTAN\n","    #we imbed the points as vectors here imidiatelly baceuse it is simpler, imidiatiley flatten 2D gridn to 1D by index = y*p + x\n","    for i, j in grid_points:\n","        if (i < low and j >= high) or (i >= high and j < low):\n","            pass\n","        else:\n","            pairs.append((i, j, num_to_generate))\n","    random.seed(config.seed)\n","    random.shuffle(pairs)\n","    div = int(config.frac_train*len(pairs))\n","    return pairs[:div], pairs[div:]\n","\n","\n","# TODO what type for model?\n","def full_loss(config : Config, model: Transformer, data):\n","    '''Takes the cross entropy loss of the model on the data'''\n","    # Take the final position only\n","    logits = model(data)[:, -1]\n","    #try\n","    labels = t.tensor([config.fn(i, j) for i, j, _ in data]).to(config.device)\n","    return helpers.cross_entropy_high_precision(logits, labels)\n","\"\"\"\n","def full_loss(config: Config, model: nn.Module, data):\n","    # 1) list -> tensor\n","    #tok = t.tensor(data, dtype=t.long, device=config.device)     # (B, 3)\n","\n","    # 2) forward\n","    logits = model(data)[:, -1]                                          # (B, d_out)\n","\n","    # 3) hard labels (your bucket fn)\n","    labels = t.tensor([config.fn(i, j) for i, j, _ in data], device=config.device)\n","\n","    # 4) per-sample CE\n","    ce_each = helpers.cross_entropy_high_precision(logits, labels)  # (B,)\n","\n","    data = t.tensor(data, dtype=t.long, device=config.device)\n","    # 5) ratio & boundary distance\n","    x = data[:, 0].float()\n","    y = data[:, 1].float().clamp_min(1)                           # avoid /0\n","    r = x / y\n","\n","    TH = config.thresh_tensor\n","    k = t.bucketize(r, TH)                                   # (B,)\n","    lower = t.where(k == 0, t.zeros_like(r), TH[k-1])\n","    upper = TH[k]\n","    width = (upper - lower).clamp_min(1e-8)\n","    d = t.minimum(r - lower, upper - r) / width                  # [0, 0.5]\n","\n","    # 6) weight\n","    #w = 1 / (d + 1e-4)\n","    w = (1 - d).pow(2)                                       # (B,)\n","\n","    # 7) weighted CE\n","    return (w * ce_each).mean()\n","\n","\n","class Trainer:\n","    '''TODO\n","    ways this stinks:\n","    - callbacks every k epochs\n","    - training on infinite data\n","    - general abstract class w/o assumption and subclasses w/ more assumptions\n","    - check out hugging face trainer\n","    - disentangle optimization step and taking gradients\n","    - forward compatibility, e.g. batches per step\n","    '''\n","\n","    def __init__(self, config : Config, model = None) -> None:\n","        wandb.init(project = \"grokking\", config = dataclasses.asdict(config))\n","        self.model = model if model is not None else Transformer(config, use_cache=False)\n","        self.model.to(config.device)\n","        self.optimizer = optim.AdamW(self.model.parameters(), lr = config.lr, weight_decay=config.weight_decay, betas=(0.9, 0.98))\n","        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lambda step: min(step/10, 1)) # TODO make this a config option\n","        self.run_name = f\"grok_{int(time.time())}\"\n","        self.train, self.test = gen_train_test(config = config)\n","        self.metrics_dictionary = defaultdict(dict) # so we can safely call 'update' on keys\n","        print('training length = ', len(self.train))\n","        print('testing length = ', len(self.test))\n","        self.train_losses = []\n","        self.test_losses = []\n","        self.config = config\n","\n","    def save_epoch(self, epoch, save_to_wandb = True):\n","        ''' precondition! train loss and test losses have been appended to '''\n","        save_dict = {\n","            'model': self.model.state_dict(),\n","            'train_loss': self.train_losses[-1],\n","            'test_loss': self.test_losses[-1],\n","            'epoch': epoch,\n","        }\n","        if save_to_wandb:\n","            wandb.log(save_dict)\n","            print(\"Saved epoch to wandb\")\n","        if self.config.save_models:\n","            t.save(save_dict, helpers.root/self.run_name/f\"{epoch}.pth\")\n","            print(f\"Saved model to {helpers.root/self.run_name/f'{epoch}.pth'}\")\n","        self.metrics_dictionary[epoch].update(save_dict)\n","\n","    def do_a_training_step(self, epoch: int):\n","        '''returns train_loss, test_loss'''\n","        train_loss = full_loss(config = self.config, model = self.model, data = self.train)\n","        test_loss = full_loss(config = self.config, model = self.model, data = self.test)\n","        self.train_losses.append(train_loss.item())\n","        self.test_losses.append(test_loss.item())\n","        if epoch % 100 == 0:\n","            # TODO is this ok? this was np.log, and it was barking at me ; i think np.log was being interpreted as a logging module\n","            print(f'Epoch {epoch}, train loss {t.log(train_loss).item():.4f}, test loss {t.log(test_loss).item():.4f}')\n","        train_loss.backward()\n","        self.optimizer.step()\n","        self.scheduler.step()\n","        self.optimizer.zero_grad()\n","        return train_loss, test_loss\n","\n","    def initial_save_if_appropriate(self):\n","        if self.config.save_models:\n","            os.mkdir(helpers.root/self.run_name)\n","            save_dict = {\n","                'model': self.model.state_dict(),\n","                'train_data' : self.train,\n","                'test_data' : self.test}\n","            t.save(save_dict, helpers.root/self.run_name/'init.pth')\n","\n","\n","    def post_training_save(self, save_optimizer_and_scheduler = True, log_to_wandb = True):\n","        if not self.config.save_models:\n","            os.makedirs(helpers.root/self.run_name, exist_ok=True)\n","        save_dict = {\n","            'model': self.model.state_dict(),\n","            'train_loss': self.train_losses[-1],\n","            'test_loss': self.test_losses[-1],\n","            'train_losses': self.train_losses,\n","            'test_losses': self.test_losses,\n","            'epoch': self.config.num_epochs,\n","        }\n","        if save_optimizer_and_scheduler:\n","            save_dict['optimizer'] = self.optimizer.state_dict()\n","            save_dict['scheduler'] = self.scheduler.state_dict()\n","        if log_to_wandb:\n","            wandb.log(save_dict)\n","        t.save(save_dict, helpers.root/self.run_name/f\"final.pth\")\n","        print(f\"Saved model to {helpers.root/self.run_name/f'final.pth'}\")\n","        self.metrics_dictionary[save_dict['epoch']].update(save_dict)\n","\n","\n","    def take_metrics(self, train, epoch):\n","        with t.inference_mode():\n","            def sum_sq_weights():\n","                # TODO refactor- taken from app code\n","                row = []\n","                for name, param in self.model.named_parameters():\n","                    row.append(param.pow(2).sum().item())\n","                return row\n","\n","            print('taking metrics')\n","\n","            all_data = t.tensor([(i, j, self.config.p) for i in range(self.config.p) for j in range(self.config.p)]).to(self.config.device)\n","            # TODO calculate key freqs is the most expensive part of this\n","            key_freqs = calculate_key_freqs(config = self.config, model = self.model, all_data = all_data)\n","            logits = self.model(all_data)[:, -1, :-1] # TODO i think this is equivalent to what's in the new paper?\n","            fourier_basis = make_fourier_basis(config = self.config)\n","            is_train, is_test = self.config.is_train_is_test(train = train)\n","            labels = t.tensor([self.config.fn(i, j) for i, j, _ in all_data]).to(self.config.device)\n","\n","            metrics = {\n","                'epoch': epoch,\n","                'trig_loss': calculate_trig_loss(config = self.config,\n","                    model = self.model,\n","                    train = train,\n","                    key_freqs = key_freqs,\n","                    is_test=is_test,\n","                    is_train=is_train,\n","                    labels=labels,\n","                    logits = logits,\n","                    fourier_basis=fourier_basis,\n","                    all_data=all_data),\n","                'sum_of_squared_weights': sum_sq_weights(),\n","                'excluded_loss': calculate_excluded_loss(\n","                    logits = logits,\n","                    key_freqs = key_freqs,\n","                    fourier_basis = fourier_basis,\n","                    is_train=is_train,\n","                    config = self.config,\n","                    is_test = is_test,\n","                    labels=labels),\n","                'coefficients': calculate_coefficients(p = self.config.p, logits = logits, fourier_basis = fourier_basis, key_freqs = key_freqs, device = self.config.device),\n","            }\n","            wandb.log(metrics)\n","            print(\"Logged metrics to wandb\")\n","            self.metrics_dictionary[epoch].update(metrics)\n","\n","def train_model(config: Config):\n","    world = Trainer(config = config)\n","    print(f'Run name {world.run_name}')\n","    world.initial_save_if_appropriate()\n","\n","    for epoch in range(config.num_epochs):\n","        train_loss, test_loss = world.do_a_training_step(epoch)\n","        if test_loss.item() < config.stopping_thresh:\n","            break\n","        if config.is_it_time_to_save(epoch = epoch):\n","            # TODO this also used to do a check about test loss- pretty sure not necessary\n","            world.save_epoch(epoch = epoch)\n","        if config.is_it_time_to_take_metrics(epoch = epoch):\n","            pass\n","            #world.take_metrics(epoch = epoch, train = world.train)\n","\n","    world.post_training_save(save_optimizer_and_scheduler=True)\n","    helpers.lines([world.train_losses, world.test_losses], labels=['train', 'test'], log_y=True)\n","    return world # to export the dictionary with the training metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1753716056194,"user":{"displayName":"Arsen Mikovic","userId":"15328410024087986821"},"user_tz":-120},"id":"i9KPPskNM8Yd","outputId":"167fe593-bf14-457f-c2e1-43dbbb82002a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0, 3, 2, 1, 0, 1, 0, 1]\n"]}],"source":["config = Config()\n","a, b = gen_train_test(config)\n","num = [0,0,0,0,0,0,0,0]\n","for i, j, _ in a:\n","  num[int(config.fn(i,j))] += 1\n","print(num)\n","#quite unevenly spaces, maybe have to bin more towards the zero smaller bins then for largr numbr larger bins\n","# how bout 1/2, 1, 2, 4, 8, 16,\n","# a/b = 1 there are a lot... more than n+n-1+n-2+...1 --> n**2\n","# then for a/b > n/2 there are like n/2 + 1 like not a lot tbh\n","# expecting this bad dataset to train well is stupid....\n","# 1, 1.1,1.3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":109},"id":"CDZGwWiZ9Cmh","outputId":"da31192a-a734-4d05-b2b5-1e11ca459a50"},"outputs":[{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:"]}],"source":["config = Config()\n","model = train_model(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnRhuWD8C7L2"},"outputs":[],"source":["num = 0\n","for x in range(1,8):\n","  for y in range(1,x+1):\n","    result = model.model([(x, y, 8)])[:, -1]\n","    if config.fn(x,y) != result.argmax(dim=-1).item():\n","          print(x, y, config.fn(x,y), result.argmax(dim=-1).item())\n","          num += 1\n","#print('average difference is ', error/num)\n","print((1-num/(32))*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afNHE64uGIOV"},"outputs":[],"source":["num = 0\n","borders = np.array([1.1, 1.30, 1.50 , 1.80, 2.20,  3.20 , 10.0])\n","error = 0\n","for x in range(1,113):\n","  for y in range(1,x+1):\n","    result = model.model([(x, y, 113)])[:, -1]\n","    if config.fn(x,y) != result.argmax(dim=-1).item():\n","          #print(x, y, config.fn(x,y), result.argmax(dim=-1).item())\n","          num += 1\n","#print('average difference is ', error/num)\n","print((1-num/(113*113/2))*100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tfx129VGg9i"},"outputs":[],"source":["#TRY DIVISION NOW\n","print( model.model([(80, 53, 113)])[:, -1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_2KfaeHHSFz"},"outputs":[],"source":["#sharp cuts and discontinuities, transformesr dont like\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1kDx2WSaRdvgj4R8KlfdDyBkvWgrwpvfc","timestamp":1752830718138}],"authorship_tag":"ABX9TyPUJ+j57DBEX7D9fStIBzKg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}